INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
['free_proxy.pipelines.JsonWriterPipeline']
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '114.134.187.162', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '212.118.62.84', 'port': '44551'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '200.95.174.180', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '52.68.246.231', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '149.56.45.68', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '45.6.216.79', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '40.114.121.235', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '18.179.92.228', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '138.201.223.250', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '129.213.76.9', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '192.116.142.153', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '167.99.224.142', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '163.44.165.165', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '209.97.129.32', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.18.114.142', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '163.172.86.64', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '180.211.134.238', 'port': '65205'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.164.181.164', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '47.75.230.208', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '54.36.162.123', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '54.65.86.208', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '84.1.114.154', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '71.13.112.152', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '191.252.194.156', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '159.65.9.66', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.93.3.123', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.69.34.21', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '177.92.4.70', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.57.46.109', 'port': '44331'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.57.225.45', 'port': '44331'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '94.130.14.146', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '13.92.196.150', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '177.204.85.203', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '52.69.254.83', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '94.130.20.85', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '147.135.210.114', 'port': '54566'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '89.250.157.122', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.27.30.203', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '23.97.215.153', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '139.99.106.27', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '88.99.149.188', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '66.82.144.29', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '87.255.8.38', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.137.148.99', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '139.255.57.32', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.89.71.70', 'port': '31222'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '78.134.30.75', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '91.215.140.61', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.69.77.217', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '189.108.153.90', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.235.249.3', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '51.15.86.88', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '213.235.94.6', 'port': '31323'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '46.8.117.32', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.18.121', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '202.74.242.248', 'port': '31323'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '213.6.40.142', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.139.109', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '159.224.176.205', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.21.98', 'port': '41258'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '103.205.177.44', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '119.110.69.130', 'port': '65103'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '218.186.27.31', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '78.26.207.173', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '158.69.143.77', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.217.107.8', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.137.190', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.139.107', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.158.127.9', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.22.233', 'port': '41258'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '45.76.235.248', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '111.68.108.34', 'port': '8080'}
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14705,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 16, 11, 27, 586074),
 'item_scraped_count': 72,
 'log_count/DEBUG': 74,
 'log_count/INFO': 7,
 'memusage/max': 52244480,
 'memusage/startup': 52244480,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 16, 11, 27, 79857)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'FEED_URI': './proxy_pool/original.json', 'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'FEED_FORMAT': 'json', 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '46.8.117.32', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '23.97.215.153', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '84.1.114.154', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '95.143.109.50', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.18.114.142', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.164.181.164', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '200.95.174.180', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '163.172.86.64', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '138.201.223.250', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '94.130.14.146', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '149.56.45.68', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.93.3.123', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.57.46.109', 'port': '44331'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '191.252.194.156', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '40.114.121.235', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '47.75.230.208', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '199.203.232.124', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '52.68.246.231', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '80.211.181.37', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '54.65.86.208', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '95.46.146.107', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.57.225.45', 'port': '44331'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '18.179.92.228', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '192.116.142.153', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '177.92.4.70', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '167.99.224.142', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '49.48.77.30', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '54.36.162.123', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '114.134.187.162', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '212.118.62.84', 'port': '44551'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '159.65.9.66', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '180.211.134.238', 'port': '65205'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '129.213.76.9', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.69.34.21', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '71.13.112.152', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '163.44.165.165', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '209.97.129.32', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '45.6.216.79', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '147.135.210.114', 'port': '54566'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '89.250.157.122', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '177.204.85.203', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '52.69.254.83', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '94.130.20.85', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '13.92.196.150', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '139.99.106.27', 'port': '10000'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.27.30.203', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '88.99.149.188', 'port': '31288'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '66.82.144.29', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '87.255.8.38', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.137.148.99', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '139.255.57.32', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '189.108.153.90', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '78.134.30.75', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '118.69.77.217', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '91.215.140.61', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.89.71.70', 'port': '31222'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '110.235.249.3', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '51.15.86.88', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '213.235.94.6', 'port': '31323'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.18.121', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '202.74.242.248', 'port': '31323'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '213.6.40.142', 'port': '80'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '159.224.176.205', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.139.109', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '119.110.69.130', 'port': '65103'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '103.205.177.44', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.21.98', 'port': '41258'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '218.186.27.31', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '78.26.207.173', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '158.69.143.77', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.137.190', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '178.217.107.8', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '122.183.139.107', 'port': '8080'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '185.158.127.9', 'port': '53281'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '45.76.235.248', 'port': '3128'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '81.30.22.233', 'port': '41258'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'ip': '111.68.108.34', 'port': '8080'}
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.extensions.feedexport:Stored json feed (77 items) in: ./proxy_pool/original.json
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14762,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 16, 28, 22, 807813),
 'item_scraped_count': 77,
 'log_count/DEBUG': 79,
 'log_count/INFO': 8,
 'memusage/max': 52879360,
 'memusage/startup': 52875264,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 16, 28, 22, 300433)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_FORMAT': 'json', 'SPIDER_MODULES': ['free_proxy.spiders'], 'FEED_URI': './proxy_pool/original.json', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '46.8.117.32'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '23.97.215.153'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '84.1.114.154'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '95.143.109.50'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '185.18.114.142'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '110.164.181.164'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '200.95.174.180'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '163.172.86.64'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31288', 'ip': '138.201.223.250'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31288', 'ip': '94.130.14.146'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '10000', 'ip': '149.56.45.68'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '185.93.3.123'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '44331', 'ip': '178.57.46.109'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '191.252.194.156'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '40.114.121.235'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '47.75.230.208'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '199.203.232.124'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '52.68.246.231'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '80.211.181.37'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '54.65.86.208'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '95.46.146.107'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '44331', 'ip': '178.57.225.45'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '18.179.92.228'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '192.116.142.153'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '177.92.4.70'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '167.99.224.142'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '49.48.77.30'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '10000', 'ip': '54.36.162.123'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '114.134.187.162'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '44551', 'ip': '212.118.62.84'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '159.65.9.66'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '65205', 'ip': '180.211.134.238'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '129.213.76.9'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '118.69.34.21'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '71.13.112.152'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '163.44.165.165'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '209.97.129.32'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '45.6.216.79'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '54566', 'ip': '147.135.210.114'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '89.250.157.122'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '177.204.85.203'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '52.69.254.83'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31288', 'ip': '94.130.20.85'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '13.92.196.150'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '10000', 'ip': '139.99.106.27'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '118.27.30.203'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31288', 'ip': '88.99.149.188'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '66.82.144.29'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '87.255.8.38'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '110.137.148.99'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '139.255.57.32'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '189.108.153.90'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '78.134.30.75'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '118.69.77.217'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '91.215.140.61'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31222', 'ip': '81.89.71.70'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '110.235.249.3'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '51.15.86.88'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31323', 'ip': '213.235.94.6'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '81.30.18.121'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '31323', 'ip': '202.74.242.248'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '80', 'ip': '213.6.40.142'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '159.224.176.205'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '122.183.139.109'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '65103', 'ip': '119.110.69.130'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '103.205.177.44'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '41258', 'ip': '81.30.21.98'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '218.186.27.31'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '78.26.207.173'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '158.69.143.77'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '122.183.137.190'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '178.217.107.8'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '122.183.139.107'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '53281', 'ip': '185.158.127.9'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '3128', 'ip': '45.76.235.248'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '41258', 'ip': '81.30.22.233'}
DEBUG:scrapy.core.scraper:Scraped from <200 https://free-proxy-list.net/>
{'port': '8080', 'ip': '111.68.108.34'}
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.extensions.feedexport:Stored json feed (77 items) in: ./proxy_pool/original.json
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14761,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 16, 29, 5, 333508),
 'item_scraped_count': 77,
 'log_count/DEBUG': 79,
 'log_count/INFO': 8,
 'memusage/max': 52629504,
 'memusage/startup': 52629504,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 16, 29, 4, 837664)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'SPIDER_MODULES': ['free_proxy.spiders'], 'COOKIES_ENABLED': False}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14698,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 32, 27, 970693),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52158464,
 'memusage/startup': 52158464,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 32, 27, 448162)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14743,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 33, 11, 836127),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52199424,
 'memusage/startup': 52199424,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 33, 11, 351545)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14695,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 33, 40, 37962),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52289536,
 'memusage/startup': 52289536,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 33, 39, 554014)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14747,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 86,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 34, 44, 223909),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52412416,
 'memusage/startup': 52412416,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 34, 43, 715224)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'SPIDER_MODULES': ['free_proxy.spiders'], 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'unknown protocol')]>]
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): Could not open CONNECT tunnel with proxy 14.139.156.110:9797 [{'reason': b'Service Unavailable', 'status': 503}]
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>: Could not open CONNECT tunnel with proxy 14.139.156.110:9797 [{'reason': b'Service Unavailable', 'status': 503}]
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14708,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 85,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 35, 22, 941996),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52269056,
 'memusage/startup': 52269056,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 35, 14, 720003)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'FEED_URI': 'pr.json', 'FEED_FORMAT': 'json', 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
ERROR:scrapy.core.scraper:Spider error processing <GET https://free-proxy-list.net/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiangning/Documents/work/smith/test/free_proxy/free_proxy/spiders/free_spider.py", line 52, in parse
    request = Request(url= "https://www.arrow.com/", callback=self.parse_valid(proxy), headers=self.headers)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/http/request/__init__.py", line 31, in __init__
    raise TypeError('callback must be a callable, got %s' % type(callback).__name__)
TypeError: callback must be a callable, got generator
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14728,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 37, 9, 161581),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52854784,
 'memusage/startup': 52850688,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/TypeError': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 37, 8, 107503)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'FEED_FORMAT': 'json', 'FEED_URI': 'pr.json'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
ERROR:scrapy.core.scraper:Spider error processing <GET https://free-proxy-list.net/> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/scrapy/utils/defer.py", line 102, in iter_errback
    yield next(it)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/offsite.py", line 30, in process_spider_output
    for x in result:
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/referer.py", line 339, in <genexpr>
    return (_set_referer(r) for r in result or ())
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/urllength.py", line 37, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/anaconda3/lib/python3.5/site-packages/scrapy/spidermiddlewares/depth.py", line 58, in <genexpr>
    return (r for r in result or () if _filter(r))
  File "/Users/xiangning/Documents/work/smith/test/free_proxy/free_proxy/spiders/free_spider.py", line 52, in parse
    request = Request(url= "https://www.arrow.com/", callback=self.parse_valid(proxy), headers=self.headers)
  File "/Users/xiangning/Documents/work/smith/test/free_proxy/free_proxy/spiders/free_spider.py", line 62, in parse_valid
    with open('a.txt') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'a.txt'
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14640,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 39, 33, 828862),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52731904,
 'memusage/startup': 52731904,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/FileNotFoundError': 1,
 'start_time': datetime.datetime(2018, 6, 21, 17, 39, 33, 447436)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'FEED_FORMAT': 'json', 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'FEED_URI': 'pr.json', 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'SSL23_GET_SERVER_HELLO', 'unknown protocol')]>]
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): Could not open CONNECT tunnel with proxy 14.139.156.110:9797 [{'reason': b'Service Unavailable', 'status': 503}]
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>: [<twisted.python.failure.Failure OpenSSL.SSL.Error: [('SSL routines', 'ssl23_read', 'ssl handshake failure')]>]
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/scrapy.core.downloader.handlers.http11.TunnelError': 1,
 'downloader/exception_type_count/twisted.web._newclient.ResponseNeverReceived': 2,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14647,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 85,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 40, 59, 421263),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52592640,
 'memusage/startup': 52592640,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/scrapy.core.downloader.handlers.http11.TunnelError': 1,
 'retry/reason_count/twisted.web._newclient.ResponseNeverReceived': 1,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 40, 56, 523917)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_URI': 'pr.json', 'COOKIES_ENABLED': False, 'FEED_FORMAT': 'json', 'BOT_NAME': 'free_proxy', 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6044
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.extensions.logstats:Crawled 1 pages (at 1 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 180.0 seconds..
INFO:scrapy.extensions.logstats:Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.extensions.logstats:Crawled 1 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_URI': 'pr.json', 'DOWNLOAD_TIMEOUT': 10, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy', 'FEED_FORMAT': 'json'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6045
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14757,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 85,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 50, 38, 511190),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52613120,
 'memusage/startup': 52613120,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 50, 7, 924941)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_URI': 'pr.json', 'COOKIES_ENABLED': False, 'FEED_FORMAT': 'json', 'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 10}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6045
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14712,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 85,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 51, 35, 689395),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52736000,
 'memusage/startup': 52731904,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 51, 5, 92578)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_URI': 'pr.json', 'COOKIES_ENABLED': False, 'FEED_FORMAT': 'json', 'DOWNLOAD_TIMEOUT': 10, 'BOT_NAME': 'free_proxy', 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6045
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 10.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14745,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 72,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 52, 51, 391430),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52789248,
 'memusage/startup': 52789248,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 52, 20, 690826)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6045
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14659,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 72,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 53, 39, 262500),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52391936,
 'memusage/startup': 52391936,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 53, 23, 688109)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'DOWNLOAD_TIMEOUT': 5}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6045
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.arrow.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.arrow.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.arrow.com/> (failed 3 times): User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
ERROR:scrapy.core.scraper:Error downloading <GET https://www.arrow.com/>
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 1384, in _inlineCallbacks
    result = result.throwExceptionIntoGenerator(g)
  File "/anaconda3/lib/python3.5/site-packages/twisted/python/failure.py", line 422, in throwExceptionIntoGenerator
    return g.throw(self.type, self.value, self.tb)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/middleware.py", line 43, in process_request
    defer.returnValue((yield download_func(request=request,spider=spider)))
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/anaconda3/lib/python3.5/site-packages/scrapy/core/downloader/handlers/http11.py", line 351, in _cb_timeout
    raise TimeoutError("Getting %s took longer than %s seconds." % (url, timeout))
twisted.internet.error.TimeoutError: User timeout caused connection failure: Getting https://www.arrow.com/ took longer than 5.0 seconds..
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1451,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14750,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 72,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 17, 54, 27, 629532),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52215808,
 'memusage/startup': 52215808,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 17, 54, 12, 63114)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy', 'DOWNLOAD_TIMEOUT': 5, 'COOKIES_ENABLED': False}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.octopart.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.octopart.com/> (failed 1 times): User timeout caused connection failure.
DEBUG:scrapy.core.engine:Crawled (403) <GET https://www.octopart.com/> (referer: https://free-proxy-list.net/)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://www.octopart.com/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 1086,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 170096,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/403': 1,
 'dupefilter/filtered': 68,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 3, 24, 354639),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 5,
 'log_count/INFO': 8,
 'memusage/max': 52400128,
 'memusage/startup': 52396032,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 21, 18, 3, 14, 820325)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'COOKIES_ENABLED': False, 'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.octopart.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.octopart.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.octopart.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.octopart.com/> (failed 2 times): User timeout caused connection failure: Getting https://www.octopart.com/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Gave up retrying <GET https://www.octopart.com/> (failed 3 times): User timeout caused connection failure.
ERROR:scrapy.core.scraper:Error downloading <GET https://www.octopart.com/>: User timeout caused connection failure.
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 3,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 3,
 'downloader/request_bytes': 1460,
 'downloader/request_count': 4,
 'downloader/request_method_count/GET': 4,
 'downloader/response_bytes': 14764,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 68,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 8, 50, 43556),
 'log_count/DEBUG': 6,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52293632,
 'memusage/startup': 52293632,
 'request_depth_max': 1,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/max_reached': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 4,
 'scheduler/dequeued/memory': 4,
 'scheduler/enqueued': 4,
 'scheduler/enqueued/memory': 4,
 'start_time': datetime.datetime(2018, 6, 21, 18, 8, 34, 449849)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://www.octopart.com/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://www.octopart.com/> (failed 1 times): User timeout caused connection failure: Getting https://www.octopart.com/ took longer than 5.0 seconds..
DEBUG:scrapy.core.engine:Crawled (403) <GET https://www.octopart.com/> (referer: https://free-proxy-list.net/)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://www.octopart.com/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 1,
 'downloader/request_bytes': 1086,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 170036,
 'downloader/response_count': 2,
 'downloader/response_status_count/200': 1,
 'downloader/response_status_count/403': 1,
 'dupefilter/filtered': 68,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 10, 7, 661379),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 5,
 'log_count/INFO': 8,
 'memusage/max': 52256768,
 'memusage/startup': 52256768,
 'request_depth_max': 1,
 'response_received_count': 2,
 'retry/count': 1,
 'retry/reason_count/twisted.internet.error.TimeoutError': 1,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 21, 18, 9, 58, 225168)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'DOWNLOAD_TIMEOUT': 5, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14748,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 65,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 14, 22, 236608),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52166656,
 'memusage/startup': 52166656,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 14, 21, 159295)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14799,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 65,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 15, 6, 611980),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52342784,
 'memusage/startup': 52342784,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 15, 6, 125192)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'DOWNLOAD_TIMEOUT': 5, 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14766,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 65,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 16, 9, 182166),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52289536,
 'memusage/startup': 52289536,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 16, 8, 735007)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 5}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14748,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 65,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 16, 20, 921549),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52191232,
 'memusage/startup': 52191232,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 16, 20, 429582)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14728,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 16, 59, 650562),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52338688,
 'memusage/startup': 52338688,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 16, 59, 187131)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy', 'DOWNLOAD_TIMEOUT': 5, 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14730,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 18, 51, 11004),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52391936,
 'memusage/startup': 52391936,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 18, 50, 561704)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14752,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 65,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 19, 10, 573633),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52219904,
 'memusage/startup': 52219904,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 19, 10, 99910)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'DOWNLOAD_TIMEOUT': 5, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14725,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 66,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 24, 23, 283501),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52391936,
 'memusage/startup': 52391936,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 24, 22, 729088)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 5, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14669,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 66,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 24, 51, 258499),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52281344,
 'memusage/startup': 52281344,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 24, 50, 779914)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'DOWNLOAD_TIMEOUT': 5, 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14733,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 66,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 25, 27, 125869),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52379648,
 'memusage/startup': 52379648,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 25, 26, 652026)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'DOWNLOAD_TIMEOUT': 5, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://free-proxy-list.net/> (failed 1 times): User timeout caused connection failure.
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://free-proxy-list.net/> (failed 2 times): User timeout caused connection failure: Getting https://free-proxy-list.net/ took longer than 5.0 seconds..
DEBUG:scrapy.core.engine:Crawled (403) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://free-proxy-list.net/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/request_bytes': 1014,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 2621,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 25, 47, 137170),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 8,
 'memusage/max': 52547584,
 'memusage/startup': 52543488,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 21, 18, 25, 33, 654890)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://free-proxy-list.net/> (failed 1 times): User timeout caused connection failure: Getting https://free-proxy-list.net/ took longer than 5.0 seconds..
DEBUG:scrapy.downloadermiddlewares.retry:Retrying <GET https://free-proxy-list.net/> (failed 2 times): User timeout caused connection failure: Getting https://free-proxy-list.net/ took longer than 5.0 seconds..
DEBUG:scrapy.core.engine:Crawled (403) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://free-proxy-list.net/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/exception_count': 2,
 'downloader/exception_type_count/twisted.internet.error.TimeoutError': 2,
 'downloader/request_bytes': 1014,
 'downloader/request_count': 3,
 'downloader/request_method_count/GET': 3,
 'downloader/response_bytes': 2613,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 27, 27, 207973),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 4,
 'log_count/INFO': 8,
 'memusage/max': 52318208,
 'memusage/startup': 52314112,
 'response_received_count': 1,
 'retry/count': 2,
 'retry/reason_count/twisted.internet.error.TimeoutError': 2,
 'scheduler/dequeued': 3,
 'scheduler/dequeued/memory': 3,
 'scheduler/enqueued': 3,
 'scheduler/enqueued/memory': 3,
 'start_time': datetime.datetime(2018, 6, 21, 18, 27, 13, 20175)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'DOWNLOAD_TIMEOUT': 5, 'SPIDER_MODULES': ['free_proxy.spiders'], 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14725,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 29, 31, 614080),
 'log_count/DEBUG': 2,
 'log_count/INFO': 7,
 'memusage/max': 52387840,
 'memusage/startup': 52387840,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 29, 31, 260635)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (403) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://free-proxy-list.net/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2627,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 30, 59, 641556),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 2,
 'log_count/INFO': 8,
 'memusage/max': 52383744,
 'memusage/startup': 52383744,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 30, 55, 82443)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'DOWNLOAD_TIMEOUT': 5, 'SPIDER_MODULES': ['free_proxy.spiders'], 'COOKIES_ENABLED': False, 'BOT_NAME': 'free_proxy', 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (403) <GET https://free-proxy-list.net/> (referer: None)
INFO:scrapy.spidermiddlewares.httperror:Ignoring response <403 https://free-proxy-list.net/>: HTTP status code is not handled or not allowed
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2614,
 'downloader/response_count': 1,
 'downloader/response_status_count/403': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 31, 25, 469804),
 'httperror/response_ignored_count': 1,
 'httperror/response_ignored_status_count/403': 1,
 'log_count/DEBUG': 2,
 'log_count/INFO': 8,
 'memusage/max': 52293632,
 'memusage/startup': 52293632,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 31, 20, 400771)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'DOWNLOAD_TIMEOUT': 5, 'SPIDER_MODULES': ['free_proxy.spiders'], 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'BOT_NAME': 'free_proxy'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET http://ip.filefab.com/index.php> (referer: None)
ERROR:scrapy.core.scraper:Spider error processing <GET http://ip.filefab.com/index.php> (referer: None)
Traceback (most recent call last):
  File "/anaconda3/lib/python3.5/site-packages/twisted/internet/defer.py", line 653, in _runCallbacks
    current.result = callback(current.result, *args, **kw)
  File "/Users/xiangning/Documents/work/smith/test/free_proxy/free_proxy/spiders/try_spider.py", line 46, in parse
    print(response.css('table.table-striped.table-bordered').css('tr')[1].css('td::text').extract())
  File "/anaconda3/lib/python3.5/site-packages/parsel/selector.py", line 61, in __getitem__
    o = super(SelectorList, self).__getitem__(pos)
IndexError: list index out of range
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 342,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 2137,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 32, 10, 711605),
 'log_count/DEBUG': 2,
 'log_count/ERROR': 1,
 'log_count/INFO': 7,
 'memusage/max': 52178944,
 'memusage/startup': 52178944,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'spider_exceptions/IndexError': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 32, 8, 398866)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'NEWSPIDER_MODULE': 'free_proxy.spiders', 'COOKIES_ENABLED': False, 'BOT_NAME': 'free_proxy', 'SPIDER_MODULES': ['free_proxy.spiders'], 'FEED_FORMAT': 'json', 'DOWNLOAD_TIMEOUT': 5, 'FEED_URI': './proxy_pool/original.json'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14710,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 59,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 51, 6, 789221),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52723712,
 'memusage/startup': 52723712,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 51, 6, 317484)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'FEED_URI': './proxy_pool/original.json', 'FEED_FORMAT': 'json', 'SPIDER_MODULES': ['free_proxy.spiders'], 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy', 'COOKIES_ENABLED': False, 'NEWSPIDER_MODULE': 'free_proxy.spiders'}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14721,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 61,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 18, 53, 30, 701595),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52776960,
 'memusage/startup': 52768768,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 18, 53, 30, 228356)}
INFO:scrapy.core.engine:Spider closed (finished)
INFO:scrapy.utils.log:Scrapy 1.5.0 started (bot: free_proxy)
INFO:scrapy.utils.log:Versions: lxml 4.2.1.0, libxml2 2.9.8, cssselect 1.0.3, parsel 1.4.0, w3lib 1.19.0, Twisted 18.4.0, Python 3.5.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:11:22) - [GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)], pyOpenSSL 18.0.0 (OpenSSL 1.0.2o  27 Mar 2018), cryptography 2.2.2, Platform Darwin-17.5.0-x86_64-i386-64bit
INFO:scrapy.crawler:Overridden settings: {'COOKIES_ENABLED': False, 'FEED_URI': 'valid.json', 'NEWSPIDER_MODULE': 'free_proxy.spiders', 'FEED_FORMAT': 'json', 'DOWNLOAD_TIMEOUT': 5, 'BOT_NAME': 'free_proxy', 'SPIDER_MODULES': ['free_proxy.spiders']}
INFO:scrapy.middleware:Enabled extensions:
['scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.memusage.MemoryUsage',
 'scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats']
INFO:scrapy.middleware:Enabled downloader middlewares:
['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
INFO:scrapy.middleware:Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
INFO:scrapy.middleware:Enabled item pipelines:
[]
INFO:scrapy.core.engine:Spider opened
INFO:scrapy.extensions.logstats:Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
DEBUG:scrapy.extensions.telnet:Telnet console listening on 127.0.0.1:6046
DEBUG:scrapy.core.engine:Crawled (200) <GET https://free-proxy-list.net/> (referer: None)
DEBUG:scrapy.dupefilters:Filtered duplicate request: <GET https://free-proxy-list.net/> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)
INFO:scrapy.core.engine:Closing spider (finished)
INFO:scrapy.statscollectors:Dumping Scrapy stats:
{'downloader/request_bytes': 338,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'downloader/response_bytes': 14688,
 'downloader/response_count': 1,
 'downloader/response_status_count/200': 1,
 'dupefilter/filtered': 63,
 'finish_reason': 'finished',
 'finish_time': datetime.datetime(2018, 6, 21, 19, 21, 5, 593193),
 'log_count/DEBUG': 3,
 'log_count/INFO': 7,
 'memusage/max': 52699136,
 'memusage/startup': 52699136,
 'request_depth_max': 1,
 'response_received_count': 1,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2018, 6, 21, 19, 21, 5, 101093)}
INFO:scrapy.core.engine:Spider closed (finished)
